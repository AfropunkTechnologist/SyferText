{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with SyferText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Plan\n",
    "\n",
    "1. How to use SyferText pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# Some imports\n",
    "import syft as sy\n",
    "from syft.generic.string import String\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import syfertext\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import csv\n",
    "#from sklearn.model_selection import train_test_split\n",
    "hook = sy.TorchHook(torch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the work environement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some PySyft workers\n",
    "me = hook.local_worker\n",
    "bob = sy.VirtualWorker(hook, id = 'bob')\n",
    "alice = sy.VirtualWorker(hook, id = 'alice')\n",
    "crypto_provider = sy.VirtualWorker(hook, id = 'crypto_provider')\n",
    "\n",
    "# Create a summary writer for logging performance with tensorboard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Create a Language object with SyferText\n",
    "nlp = syfertext.load('en_core_web_lg', owner = me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -3. Download the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the dataset file\n",
    "dataset_path = '../../datasets/imdb/imdb.csv'\n",
    "\n",
    "# store the dataset as a list of dictionaries\n",
    "# each dictionary has two keys, 'review' and 'label'\n",
    "# the 'review' element is a PySyft String\n",
    "# the 'label' element is an integer with 1 for 'positive'\n",
    "# and 0 for 'negative' review\n",
    "dataset_local = []\n",
    "\n",
    "with open(dataset_path, 'r') as dataset_file:\n",
    "    \n",
    "    # Create a csv reader object\n",
    "    reader = csv.DictReader(dataset_file)\n",
    "    \n",
    "    for elem in reader:\n",
    "        \n",
    "        # Create one entry\n",
    "        example = dict(review = String(elem['review']),\n",
    "                       label = 1 if elem['sentiment'] == 'positive' else 0\n",
    "                      )\n",
    "        \n",
    "        # add to the local dataset\n",
    "        dataset_local.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<VirtualWorker id:me #objects:0>, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_local[0]['review'].owner, dataset_local[0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1. Send the Dataset to Remote Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cut the dataset into two equal parts and send each part to a different worker simulating two remote datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local splits\n",
    "dataset_bob = dataset_local[:25000]\n",
    "dataset_alice = dataset_local[25000:]\n",
    "\n",
    "# Send the content of each split\n",
    "for example in dataset_bob:\n",
    "    example['review'] = example['review'].send(bob)\n",
    "    \n",
    "    one_hot_label = torch.zeros(2).scatter(0, torch.Tensor([example['label']]).long(), 1)\n",
    "    example['label'] = one_hot_label.send(bob)\n",
    "    \n",
    "for example in dataset_alice:\n",
    "    example['review'] = example['review'].send(alice)\n",
    "    \n",
    "    one_hot_label = torch.zeros(2).scatter(0, torch.Tensor([example['label']]).long(), 1)\n",
    "    example['label'] = one_hot_label.send(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'syft.generic.pointers.string_pointer.StringPointer'>\n",
      "<VirtualWorker id:bob #objects:50000>\n",
      "<class 'torch.Tensor'>\n",
      "<VirtualWorker id:bob #objects:50000>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset_bob[0]['review']))\n",
    "print(dataset_bob[0]['review'].location)\n",
    "\n",
    "print(type(dataset_bob[0]['label']))\n",
    "print(dataset_bob[0]['label'].location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Create a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should go into the __getitem__ method of the dataloader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetIMDB(Dataset):\n",
    "    \n",
    "    def __init__(self, sets, workers, crypto_provider, nlp):\n",
    "        \n",
    "        self.sets = sets\n",
    "        self.crypto_provider = crypto_provider\n",
    "        self.workers = workers\n",
    "    \n",
    "        # Create a single dataset unifying all datasets\n",
    "        self._create_dataset()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # get the example\n",
    "        example = self.dataset[index]\n",
    "        \n",
    "        # Tokenize the string and get a doc pointer\n",
    "        doc_ptr = nlp(example['review'])\n",
    "        \n",
    "        # Get the encrypted vector embedding for the document\n",
    "        vector_enc = doc_ptr.get_encrypted_vector(bob, \n",
    "                                                  alice, \n",
    "                                                  crypto_provider = self.crypto_provider,\n",
    "                                                  requires_grad = True\n",
    "                                                 )\n",
    "        \n",
    "        \n",
    "        # Encrypte the target label\n",
    "        label_enc = example['label'].fix_precision().share(bob, \n",
    "                                                           alice, \n",
    "                                                           crypto_provider = self.crypto_provider,\n",
    "                                                           requires_grad = True\n",
    "                                                          ).get()\n",
    "        \n",
    "        return vector_enc, label_enc\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        # The size of the combined datasets\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def _create_dataset(self):\n",
    "        \"\"\"Create a single dataset unifying examples from all remote datasets\n",
    "        \"\"\"\n",
    "        # Initialize the dataset\n",
    "        self.dataset = []\n",
    "      \n",
    "        # populate the dataset list\n",
    "        for dataset in self.sets:\n",
    "            for example in dataset:\n",
    "                self.dataset.append(example)\n",
    "                \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \n",
    "        # Unzip the batch\n",
    "        vectors, targets = list(zip(*batch))\n",
    "\n",
    "        # concatenate the vectors\n",
    "        vectors = torch.stack(vectors)\n",
    "        \n",
    "        #concatenate the labels\n",
    "        targets = torch.stack(targets)\n",
    "        \n",
    "        return vectors, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "for example in dataset_local:\n",
    "    \n",
    "    # Tokenize\n",
    "    doc = nlp(example['review'])\n",
    "    for token in doc:\n",
    "        \n",
    "        try:\n",
    "            print(token.vector)\n",
    "            \n",
    "        except KeyError: # Temporary fix while Bachir resolves the issue\n",
    "            print(np.zeros(300))\n",
    "    break\n",
    "    # Get the doc vector\n",
    "    #vector = doc.vector\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "#learning_rate = 0.01\n",
    "#batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Instantiate a Dataset object\n",
    "trainset = DatasetIMDB(sets = [dataset_bob,\n",
    "                               dataset_alice],\n",
    "                       workers = [bob, alice],\n",
    "                       crypto_provider = crypto_provider,\n",
    "                       nlp = nlp\n",
    "                      )\n",
    "\n",
    "# Instantiate the DataLoader object\n",
    "trainloader = DataLoader(trainset, shuffle = True,\n",
    "                         batch_size = batch_size, num_workers = 0, \n",
    "                         collate_fn = trainset.collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an Encrypted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.fc = torch.nn.Linear(in_features, out_features)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        #preds = self.sig(logits)\n",
    "        probs = F.relu(logits)\n",
    "        \n",
    "        return probs, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniitialize and encrypt the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(in_features = 300, out_features = 2)\n",
    "\n",
    "# Apply SMPC encryption\n",
    "classifier = classifier.fix_precision().share(bob, alice, \n",
    "                                              crypto_provider = crypto_provider,\n",
    "                                              requires_grad = True\n",
    "                                              )\n",
    "print(classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptim = optim.Adam(params = classifier.parameters(),\\n                   lr = learning_rate)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optim = optim.SGD(params = classifier.parameters(),\n",
    "                   lr = learning_rate).fix_precision()\n",
    "'''\n",
    "optim = optim.Adam(params = classifier.parameters(),\n",
    "                   lr = learning_rate)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 56.25 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 65.625 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 53.125 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 53.125 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 40.625 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 50.0 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 37.5 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 65.625 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 43.75 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 53.125 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 56.25 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 43.75 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 56.25 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 68.75 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 53.125 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 46.875 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 43.75 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 56.25 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 46.875 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 56.25 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 28.125 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 46.875 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 62.5 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 43.75 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 59.375 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 46.875 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 53.125 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 53.125 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 53.125 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 31.25 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 53.125 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 50.0 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 56.25 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 56.25 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 50.0 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 50.0 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 50.0 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 46.875 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 43.75 %\n",
      "Ep 0 |  train loss: -144115194920960.0, train accuracy 59.375 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-561093a2c799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# 1). Zero out previous gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c151597a2417>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                   \u001b[0malice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                                   \u001b[0mcrypto_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrypto_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                                   \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                                                  )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/SyferText/syfertext/pointers/doc_pointer.py\u001b[0m in \u001b[0;36mget_encrypted_vector\u001b[0;34m(self, crypto_provider, requires_grad, *workers)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# I call get because the returned object is a PointerTensor to the AdditiveSharedTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdoc_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/syft/generic/pointers/pointer_tensor.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, user, reason, deregister_ptr)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mobject\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0mto\u001b[0m \u001b[0;31m#on a remote machine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectPointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderegister_ptr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mderegister_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# TODO: remove these 3 lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/syft/generic/pointers/object_pointer.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, user, reason, deregister_ptr)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# get tensor from location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_at_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# Remove this pointer by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrequest_obj\u001b[0;34m(self, obj_id, location, user, reason)\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \"\"\"\n\u001b[0;32m--> 628\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjectRequestMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_pending_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# Step 1: route message to appropriate function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_router\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# Step 2: Serialize the message to simple python objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrespond_to_obj_req\u001b[0;34m(self, request_msg)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \"\"\"\n\u001b[1;32m    567\u001b[0m         \u001b[0mobj_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"allow\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mGetNotPermittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mget_obj\u001b[0;34m(self, obj_id)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;31m# and sent back, so it will be GCed but remote data is any shouldn't be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"child\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"set_garbage_collect_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_garbage_collect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Automatically attaching gradient functions if they are defined in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# gradients module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Backward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# print(f\"getattribute {name}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for iter, (vectors, targets) in enumerate(trainloader):\n",
    "\n",
    "        # 1). Zero out previous gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # 2). predict sentiments\n",
    "        probs, logits = classifier(vectors)\n",
    "        #print(preds.shape, logits.shape)\n",
    "        # 3). Compute loss and accuracy\n",
    "        #loss = torch.nn.BCEWithLogitsLoss().fix_precision()(preds, train_labels.unsqueeze(1))\n",
    "        #loss = torch.nn.BCEWithLogitsLoss()(preds, targets.unsqueeze(1))\n",
    "        \n",
    "        #loss = targets * torch.log(preds) + (1 - targets) * torch.log((1 - preds))\n",
    "        #loss = - torch.mean(loss)\n",
    "        loss = ((probs -  targets)**2).sum()#.refresh()# / len(train_data)\n",
    "        #loss = ((probs -  targets)**2).sum() / batch_size\n",
    "\n",
    "        # Get the predicted labels\n",
    "        #pred_labels = (preds > 0.5).long().squeeze(1)\n",
    "        #accuracy = torch.mean((pred_labels == targets).float()) * 100\n",
    "        #print(probs.get())\n",
    "\n",
    "        preds = probs.argmax(dim=1)\n",
    "        targets = targets.argmax(dim=1)\n",
    "        accuracy = preds.eq(targets).sum()\n",
    "        accuracy = accuracy.get().float_precision()\n",
    "        accuracy = 100 * (accuracy / batch_size)\n",
    "        # 4). Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5). Update weights\n",
    "        optim.step()\n",
    "        \n",
    "        # print loss\n",
    "        #print(loss.get().float_precision())\n",
    "        \n",
    "        # decrypt the loss\n",
    "        loss = loss.get().float_precision()\n",
    "        #loss = loss / batch_size\n",
    "        print(f\"Ep {epoch} |  train loss: {loss}, train accuracy {accuracy:2} %\")\n",
    "        #print(f\"Ep {epoch} |  train loss: {loss}, train accuracy {accuracy:2} %\")\n",
    "        \n",
    "        # Log to tensorboard\n",
    "        writer.add_scalar('train/loss', loss, epoch * len(trainloader) + iter )\n",
    "        writer.add_scalar('train/acc', accuracy, epoch * len(trainloader) + iter )\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "1. **Create a pipe element to perform polarity cutoff:**\n",
    "\n",
    "```\n",
    "word_polarity = pass # dict of the form word: polarity\n",
    "polarity_filter = PolarityFilter(data = word_polarity,\n",
    "                                 max_pol = 1.3, \n",
    "                                 min_pol = -1.3, # tokens whos polarity between [min_pol, max_pol] \n",
    "                                                 # will be tagged 'in_doc_vectors' = False\n",
    "                                 tag = 'in_doc_vector' # the tag in which to store the True of False result\n",
    "                                )\n",
    "\n",
    "# create an element that computes a doc vector\n",
    "# only by considering the elements tagged as \n",
    "# sepcifified by 'tag'\n",
    "doc_vectorizer = DocVectorizer(tag = 'in_doc_vector')\n",
    "\n",
    "nlp.add_pipe(polarity_filter)\n",
    "nlp.add_pipe(doc_vectorizer)\n",
    "```\n",
    "\n",
    "2. **create a module in SyferText where the Dataset object for IMDB movie reviews can be imported:**\n",
    "\n",
    "(or DatasetIMDB can just return a list of sets, each is of the form [{'review': <string>, 'sentiment': int}, ...])\n",
    "and then we create the dataset object out of this\n",
    "    \n",
    "```\n",
    "from syfertext.datasets.imdb import DatasetIMDB\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# This loads the IMDB PyTorch Dataset object\n",
    "# it will download the dataset, load it locally \n",
    "# then sends it to the specified workers \n",
    "# we can specify the proportion each worker takes\n",
    "# of each of the classes. for instance\n",
    "# bob here takes 30% of positive examples from the \n",
    "# original dataset and 20% of its negative examples\n",
    "trainset = DatasetIMDB(bob, alice, \n",
    "                       dist = {'bob': {'pos' : 30, 'neg': 20},\n",
    "                               'alice': {'pos' 70, 'neg': 50}},\n",
    "                       mode = 'train'\n",
    "                       pipes = [polarity_filter, doc_vectorizer]\n",
    "                      )\n",
    "                      \n",
    "valset = DatasetIMDB(bob, alice, \n",
    "                     dist = {'bob': {'pos' : 50, 'neg': 50},\n",
    "                             'alice': {'pos' 50, 'neg': 50}},\n",
    "                     mode = 'val',\n",
    "                     pipes = [polarity_filter, doc_vectorizer]\n",
    "                    )\n",
    "                            \n",
    "# Create a torch data loaders\n",
    "trainloader = DataLoader(trainset, shuffle = True,\n",
    "                         batchsize = 16, workers = 2)\n",
    "                  \n",
    "valloader = DataLoader(valset, shuffle = False,\n",
    "                         batchsize = 16, workers = 2)\n",
    "     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
