{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started: Local Tokenization\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn about a very simple use case when using SyferText to tokenize a python `str` or a PySyft `String` residing on a local PySyft worker (No remote workers are involved). \n",
    "\n",
    "In addition to tokenization, you will also learn how to access the vector embedding of each resulting token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Alan Aboudib`  -> [@alan_aboudib](https://twitter.com/alan_aboudib) (Twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "## 1. `SyferText`'s local architecture\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SyferText's architecture is inspired by that of [spaCy](https://spacy.io/). If you are familiar with spaCy, you should feel familiar with the way SyferText works.\n",
    "\n",
    "However, unlike spaCy, SyferText is designed to leverage [PySyft](https://github.com/OpenMined/PySyft)'s ability to work with remote workers and of course to enforce privacy when designing NLP deep learning models.\n",
    "\n",
    "In this tutorial, we will focus on the local worker case. Using SyferText for remote string tokenizations is  discussed in [another tutorial](https://bit.ly/37VEJ28) that you can check out.\n",
    "\n",
    "Here is the architecture of SyferText when used for tokenizing strings on the local worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SyferText architecture: local case](art/syfertext_local.png \"SyferText architecture on the local worker\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice from the above figure, a few steps are involved in the process of tokenization:\n",
    "\n",
    "1. An object of the `Language` class is instantiated when a a language model is loaded by calling the `load()` method. \n",
    "\n",
    "2. When given a PySyft `String` or a Python `str`, the `Language` object spawns a `Tokenizer` object.\n",
    "\n",
    "3. The tokenizer breaks that string down into `Token` objects. \n",
    "\n",
    "4. The `Doc` object keeps track of those tokens. \n",
    "\n",
    "In the below example, you will see what attributes such `Token` objects have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "## 2. Tokenizing a Python `str` object\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import SyferText. Since SyferText is based on PySyft, we also need to import the latter, as well as PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings (nothing to do with SyferText)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "missing ), unterminated subpattern at position 33",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2208b60b42ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msyft\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msyfertext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Anton/OpenMined/SyferText/syfertext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpointers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_pointer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocPointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsgpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserde\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mserde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anton/OpenMined/SyferText/syfertext/language.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Author: Alan Aboudib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpointers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_pointer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizerPointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anton/OpenMined/SyferText/syfertext/tokenizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpunctuations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprefix_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfix_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix_re\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtoken_exception\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTOKENIZER_EXCEPTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anton/OpenMined/SyferText/syfertext/punctuations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mTOKENIZER_INFIXES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infixes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0msuffix_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_suffix_regex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_suffixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mprefix_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_prefix_regex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prefixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0minfix_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_infix_regex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_infixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anton/OpenMined/SyferText/syfertext/utils.py\u001b[0m in \u001b[0;36mcompile_suffix_regex\u001b[0;34m(entries)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \"\"\"\n\u001b[1;32m    150\u001b[0m     \u001b[0mexpression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"|\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpiece\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"$\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpiece\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/re.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;34m\"Compile a regular expression pattern, returning a pattern object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, pattern)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0;32m--> 416\u001b[0;31m                            not nested and not items))\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 raise source.error(\"missing ), unterminated subpattern\",\n\u001b[0;32m--> 768\u001b[0;31m                                    source.tell() - start)\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosegroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: missing ), unterminated subpattern at position 33"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "import torch\n",
    "import syfertext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to hook PyTorch using the TorchHook in PySyft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will endow PyTorch with magic powers, privacy-preserving deep learning powers, such as Federated Learning, Differential Privacy, encrypted training and more. To learn more about PySyft, you can check out its awesome [tutorial notebooks](https://github.com/OpenMined/PySyft/tree/master/examples/tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every machine in PySyft is called a worker. Since we are using SyferText to tokenize a string on our local machine, then we should get an instance of the object representing that worker, let's call it 'me':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = hook.local_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to load the language model. The only language model available for the moment in SyferText is `en_core_web_lg`, which is a model for English language simplified from spaCy's language model with the same name. Check out the  properties of that model [here](https://spacy.io/models/en#en_core_web_lg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(syfertext.language.Language, <VirtualWorker id:me #objects:0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = syfertext.load('en_core_web_lg', owner = me)\n",
    "\n",
    "type(nlp), nlp.owner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the cell's output that the `nlp` variable is an object of the `Language` class, and similar to all PySyft objects, it has an owner, which is a PySyft `VirtualWorker` representing our local machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a python native `str` object and tokenize it using the `Language` object we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(syfertext.doc.Doc, <VirtualWorker id:me #objects:0>, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = 'I !am ({tokenizing a #python string!!'\n",
    "\n",
    "# Tokenization happens here\n",
    "doc = nlp(my_str)\n",
    "\n",
    "# A Doc object is returned\n",
    "type(doc), doc.owner, len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that calling the `Language` object with the `str` object we defined as an argument returns a `Doc` object (a document object). The latter is also a PySyft object that has an owner (the local worker in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get access to `Token` objects, we can simply iterate through the `Doc` object. Again, if you know spaCy, this should be familiar to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         I |  True |   0 | 1\n",
      "         ! | False |   2 | 3\n",
      "        am |  True |   3 | 5\n",
      "         ( | False |   6 | 7\n",
      "         { | False |   7 | 8\n",
      "tokenizing |  True |   8 | 18\n",
      "         a |  True |  19 | 20\n",
      "         # | False |  21 | 22\n",
      "    python |  True |  22 | 28\n",
      "    string | False |  29 | 35\n",
      "         ! | False |  35 | 36\n",
      "         ! | False |  36 | 37\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print('%10s | %5s | %3s | %s'%(token, token.space_after, token.start_pos,token.stop_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Token objects can be used to get access to the underlying string, to whether that string is followed by a space or not in the original sentence, and to the string's hash. We can also get the vector embedding for each token using the vector attribute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get off-the-shelf token vectors of the third word of the original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing\n"
     ]
    }
   ],
   "source": [
    "print(doc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.7238e-01,  3.0606e-01, -3.4498e-01,  2.1782e-01,  3.7487e-01,\n",
       "        9.4773e-01,  1.7543e-01,  3.8181e-01, -3.3346e-03, -1.4996e+00,\n",
       "        2.5235e-01, -3.2070e-01,  2.1666e-01,  3.8842e-01,  4.8307e-02,\n",
       "       -2.0648e-01,  9.9902e-03, -1.3044e+00,  8.1312e-01,  1.1367e-02,\n",
       "        2.0233e-02, -6.3248e-02, -9.4263e-01,  5.4770e-01,  1.7891e-01,\n",
       "        5.5033e-01,  8.9688e-01, -6.5181e-01,  2.3126e-01, -4.4428e-01,\n",
       "        3.4075e-01, -1.7907e-01,  3.2387e-01, -5.2024e-01, -2.4935e-01,\n",
       "        2.1229e-01,  5.6182e-01, -4.8586e-01, -2.9836e-02,  2.2277e-01,\n",
       "       -4.1493e-01, -1.4441e-01, -6.6352e-01,  3.1543e-01, -4.8249e-01,\n",
       "        5.2673e-01, -8.1652e-04, -3.6682e-02, -3.3230e-01, -2.8628e-01,\n",
       "       -4.3110e-01,  3.3238e-01,  8.8027e-01,  9.8399e-02, -1.1932e-01,\n",
       "       -2.3699e-01, -6.4863e-02,  1.6402e-01,  1.1817e-01,  3.6778e-01,\n",
       "       -2.7983e-01,  7.4529e-02, -4.2815e-01, -6.0214e-01, -1.1596e-01,\n",
       "        7.1352e-01, -1.7460e-01,  6.5253e-02,  3.3433e-01, -1.9684e-02,\n",
       "        9.7229e-03,  3.4959e-01,  2.6797e-01, -2.1420e-01,  1.5044e-01,\n",
       "       -3.5128e-01,  1.5695e-01, -6.8790e-02,  3.3182e-01, -2.7151e-02,\n",
       "       -2.3522e-01,  4.5312e-01,  2.7156e-01, -4.4766e-02, -5.1150e-01,\n",
       "        1.9610e-01,  7.0399e-01, -3.5833e-02, -2.4595e-01,  6.2381e-01,\n",
       "        3.0266e-01,  1.3237e-01,  9.0178e-02,  1.8300e-01, -1.4967e-01,\n",
       "       -3.5767e-02, -3.7952e-02, -5.7221e-01,  6.2528e-01, -6.6038e-01,\n",
       "        5.4164e-01, -2.3780e-02,  2.0972e-01,  2.0533e-01, -2.6838e-01,\n",
       "       -1.4951e+00,  1.8742e-01, -2.3678e-02, -6.6719e-01,  4.3744e-01,\n",
       "       -5.1503e-01,  1.8366e-01,  3.1545e-01,  2.9975e-01, -1.4297e-02,\n",
       "       -2.3128e-01, -6.1629e-02, -1.0580e-01, -5.7863e-01,  7.2153e-01,\n",
       "        2.6901e-01, -1.8458e-01, -1.3636e-01,  4.0479e-01,  3.7991e-01,\n",
       "       -7.5943e-01, -2.6825e-01, -1.3566e-01,  1.9373e-01, -2.3219e-01,\n",
       "       -7.3102e-01,  1.3321e-01, -2.9970e-01, -3.3731e-01, -1.3349e-01,\n",
       "       -7.0879e-01,  4.2758e-01, -1.7285e-01,  1.9311e-01, -2.4294e-01,\n",
       "        3.9460e-01, -5.1010e-02, -5.0347e-02,  1.9755e-01, -2.4538e-01,\n",
       "        6.4309e-02,  3.3411e-01, -6.2032e-01, -2.0517e-01, -1.0932e-01,\n",
       "        4.2709e-01, -2.2894e-01, -3.0682e-02,  9.3794e-01,  9.6059e-02,\n",
       "        6.8756e-01,  7.3396e-01, -4.5894e-01,  1.4473e-01,  6.8567e-01,\n",
       "       -4.1639e-01, -1.3938e-02,  2.6863e-01,  9.9483e-02,  5.4613e-01,\n",
       "        1.0371e-01, -1.1194e-01, -8.3771e-02,  4.2256e-01,  4.0689e-02,\n",
       "        9.0120e-02,  4.9794e-02,  4.9539e-01,  1.2655e-01,  1.8142e-01,\n",
       "       -4.3555e-01, -2.8463e-01,  3.3199e-02,  1.3404e-01,  2.0920e-01,\n",
       "       -5.9289e-01, -6.1645e-01, -5.0986e-01,  5.0639e-01, -2.0606e-01,\n",
       "        2.4200e-01,  7.6963e-02,  3.5988e-01, -4.5233e-01, -4.0543e-01,\n",
       "        6.1610e-01,  5.0449e-01, -3.6565e-01, -5.5673e-01, -1.0388e-01,\n",
       "        2.3147e-01, -1.4173e-01,  8.0930e-01,  4.9300e-02, -7.0428e-01,\n",
       "        2.1158e-01,  8.3324e-02, -2.3426e-01,  3.8489e-01,  8.3052e-02,\n",
       "       -1.5736e-01, -9.7439e-02,  1.5724e-01,  2.5548e-01,  5.6434e-02,\n",
       "        1.3938e-01,  4.2027e-01, -1.2569e-01, -4.3788e-01,  7.6042e-01,\n",
       "       -2.6184e-01, -4.2212e-02, -3.9047e-01,  1.0034e+00,  4.3571e-02,\n",
       "       -2.8107e-01,  4.6653e-01,  1.1182e+00,  1.0264e-01, -3.1728e-01,\n",
       "       -4.2450e-01, -1.1928e-01, -4.2517e-01,  9.0026e-01, -3.6536e-01,\n",
       "        6.2219e-01, -4.3648e-01,  2.4331e-01,  4.3281e-01, -4.8096e-01,\n",
       "        8.1417e-01, -7.5825e-02,  5.1990e-01,  2.8507e-02, -3.2562e-01,\n",
       "        1.5182e-01, -1.6669e-01,  1.3471e-01, -3.6499e-01,  2.6344e-01,\n",
       "       -8.3354e-01,  3.1221e-01, -5.4022e-01,  1.2047e+00, -5.7647e-01,\n",
       "       -1.3454e-01,  7.3650e-01,  1.5985e-01,  2.0703e-02, -3.4120e-01,\n",
       "       -9.3391e-02,  3.6999e-01,  4.8536e-01,  3.8693e-01,  2.0130e-02,\n",
       "        1.1123e-01,  9.9138e-02, -3.9508e-01, -3.2817e-01,  1.4008e-01,\n",
       "        8.6690e-01, -4.3267e-01, -4.2554e-04,  2.8409e-01, -2.8176e-01,\n",
       "       -8.7813e-01, -6.0634e-01,  2.6557e-01, -2.5214e-01,  8.3335e-01,\n",
       "        2.7298e-02,  3.1267e-01,  5.1685e-02,  8.7888e-02, -3.1335e-01,\n",
       "       -6.2951e-02,  4.9376e-01,  1.9186e-01,  3.3406e-01,  1.7058e-01,\n",
       "       -3.7095e-01, -9.4815e-01,  2.5458e-01,  1.8149e-01, -9.5664e-01,\n",
       "        5.6851e-01, -7.7398e-01, -1.9765e-01, -9.6032e-02, -1.0051e-01,\n",
       "        4.5288e-01, -7.2744e-01,  2.5616e-01, -7.7381e-01, -2.2065e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we tokenized a python native string, let's now do the same with a PySyft `String`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "## 3. Tokenizing a PySyft `String` object\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySyft has its own string type which is basically a wrapper around the native `str` type with additional PySyft magic such as the ability to send a string to a remote worker and to manipulate it from the comfort of the local worker. We are not going to discuss this here since we are only doing local string tokenization. \n",
    "\n",
    "Let's import the PySyft's `String` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.generic.string import String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a PySyft `String` to tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(syft.generic.string.String, <VirtualWorker id:me #objects:0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = String('I am token#izing a PySyft String object')\n",
    "\n",
    "type(my_string), my_string.owner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the PySyft `String` is owned by the local worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the `Language` object we created earlier to tokenize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         I |  True | 5943131912006430202\n",
      "        am |  True | 11728213064939857863\n",
      "     token | False | 12977505502605755571\n",
      "         # | False | 15748096671724166044\n",
      "     izing |  True | 7913766558530700748\n",
      "         a |  True | 5182201742351716208\n",
      "    PySyft |  True | 13286865392898898656\n",
      "    String |  True | 13847508276233069841\n",
      "    object | False | 10176415242575268008\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(my_string)\n",
    "\n",
    "for token in doc:\n",
    "    print('%10s | %5s | %s'%(token, token.space_after, token.orth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also be able to get the embedding vector using the `vector` attribute. Pretty convenient right? Using either a PySyft `string` or a `str` object does not change the way SyferText is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have a better sense of how SyferText works on a local worker by now. However, keep in mind that SyferText is still in its early developement phase. Things are evolving and more features will be added soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have any questions or suggestions, you can DM me on OpenMined's [slack channel](http://slack.openmined.org/), or otherwise directly on my [Twitter page](https://twitter.com/alan_aboudib)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
